{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uNz-BxRo7J2"
      },
      "source": [
        "## Day 2: HNSW Tuning and Filtering Optimization\n",
        "\n",
        "In this tutorial, we'll explore how Qdrant's HNSW algorithm affects search performance and how payload indexes can dramatically improve filtering speed. You'll learn to:\n",
        "\n",
        "- Tune our HNSW configurations\n",
        "- Compare filtering performance with and without payload indexes\n",
        "\n",
        "### 1. Installing Required Dependencies\n",
        "\n",
        "We'll need several libraries to work with Qdrant and perform our performance tests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLqe2BYoo7J4"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets qdrant-client tqdm openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyRtsu1o7J4"
      },
      "source": [
        "Now let's install the necessary libraries. These packages will enable us to:\n",
        "\n",
        "- `datasets`: Load and work with the DBpedia dataset\n",
        "- `qdrant-client`: Interact with our Qdrant vector database\n",
        "- `tqdm`: Show progress bars during data processing\n",
        "- `openai`: Generate embeddings for our queries\n",
        "- `time` - for performance timing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rid2mQMto7J4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lzandrade.TOPAZ\\AppData\\Local\\hatch\\env\\virtual\\materials\\URYRdbY_\\materials\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from qdrant_client import QdrantClient, models\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM3078nCo7J5"
      },
      "source": [
        "### 2. Connecting to Qdrant Cloud\n",
        "\n",
        "Now we'll establish a connection to our Qdrant cloud instance. Unlike the Day 1 tutorial which used an in-memory database, we're using Qdrant Cloud for this performance testing. This allows us to test with larger datasets and measure real-world performance with cloud infrastructure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZ4mXapyo7J5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "collections=[CollectionDescription(name='store'), CollectionDescription(name='my_first_collection'), CollectionDescription(name='my_collection'), CollectionDescription(name='day0_first_system'), CollectionDescription(name='dbpedia_100K'), CollectionDescription(name='dev_vectors'), CollectionDescription(name='production_vectors')]\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import userdata\n",
        "from materials.q_drant_client import client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYex1v-Jo7J5"
      },
      "source": [
        "### 3. Loading the DBpedia Dataset\n",
        "\n",
        "We'll use the DBpedia entities dataset, which contains 100K Wikipedia articles with **pre-computed embeddings** with OpenAI's `text-embedding-3-large` model with 1536-dimensional vectors (first 1536 dimensions of a `text-embedding-3-large` embedding), which is ideal for testing HNSW performance on high-dimensional data.\n",
        "\n",
        "The 100K vectors dataset allow us to see real performance differences and includes titles, text, and categories for filtering tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAJ8A3vQo7J5"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-100K\")\n",
        "\n",
        "collection_name = \"dbpedia_100K\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtiiPfM-o7J5"
      },
      "source": [
        "### 4. Creating Our Collection\n",
        "\n",
        "We're starting with `m=0` for a specific reason: **bulk upload speed**. When `m=0`, Qdrant doesn't build any HNSW connections during indexing, which makes uploading 100K vectors much faster. This is perfect for our testing workflow, where we'll:\n",
        "\n",
        "1. Upload data quickly with `m=0`\n",
        "2. Test full scan performance\n",
        "3. Update to `m=16` and test HNSW performance\n",
        "4. Compare the difference\n",
        "\n",
        "> **<font color='red'>Warning:</font>** Don't use this technique on subsequent bulk uploads. Setting `m=0` will delete the existing HNSW index. Rebuilding from scratch is slow and resource-intensive.\n",
        "\n",
        "The `strict_mode_config` with `enabled=False` and `unindexed_filtering_retrieve=True` allows us to test filtering without payload indexes, so we can measure the performance impact of adding indexes later.\n",
        "\n",
        "> Qdrant (Managed) Cloud runs in strict mode by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9IEAHOHo7J6",
        "outputId": "11f1daaf-052a-46e4-824f-0cbd2f70cd13"
      },
      "outputs": [],
      "source": [
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=1536,\n",
        "        distance=models.Distance.COSINE\n",
        "    ),\n",
        "    hnsw_config=models.HnswConfigDiff(\n",
        "        m=0, # número de nós vizinhos para cada nó\n",
        "        ef_construct=100, # quantos candidatos para cada nó\n",
        "        full_scan_threshold=10000 # abaixo disso, faz scan completo, acima disso, faz scan parcial HNSW\n",
        "    ),\n",
        "    strict_mode_config=models.StrictModeConfig(\n",
        "        enabled=False,\n",
        "        unindexed_filtering_retrieve=True  # Allow filtering without indexes\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Created collection: {collection_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-TEyP82o7J6"
      },
      "source": [
        "### 5. Exploring Our Dataset\n",
        "\n",
        "Before we start uploading data, let's examine the dataset structure to understand what we're working with. This helps us verify the data format and plan our upload strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X0Ckba0o7J6",
        "outputId": "dabb34ff-d39a-47ab-f62b-df4eca1f3670"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset info:\")\n",
        "print(ds)\n",
        "\n",
        "print(\"\\nFirst example (proper access):\")\n",
        "first_example = ds['train'][0]\n",
        "print(first_example)\n",
        "\n",
        "print(\"\\nDataset features:\")\n",
        "print(ds['train'].features)\n",
        "\n",
        "print(\"\\nAvailable columns:\")\n",
        "print(ds['train'].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nfEF0sco7J6"
      },
      "source": [
        "### 6. Bulk Uploading Points\n",
        "\n",
        "We're uploading 100K vectors in large batches (10K each) to speed up the process. With `m=0`, the upload is much faster since Qdrant isn't building HNSW connections during indexing.\n",
        "\n",
        "> **<font color='red'>Warning:</font>** Don't use this technique on subsequent bulk uploads. Setting `m=0` will delete the existing HNSW index. Rebuilding from scratch is slow and resource-intensive.\n",
        "\n",
        "The payload includes fields we'll use for filtering tests: `length` (text length) and `has_numbers` (a boolean flag). These will let us test different filter types later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV1kaqgeo7J6",
        "outputId": "57ec702a-572a-490d-9e62-c27e296096a9"
      },
      "outputs": [],
      "source": [
        "batch_size = 10000\n",
        "total_points = len(ds['train'])\n",
        "\n",
        "print(f\"Uploading {total_points} points in batches of {batch_size}\")\n",
        "\n",
        "def upload_batch_without_indexes(start_idx, end_idx):\n",
        "    points = []\n",
        "    for i in range(start_idx, min(end_idx, total_points)):\n",
        "        example = ds['train'][i]\n",
        "\n",
        "        # Get the embedding\n",
        "        embedding = example['text-embedding-3-large-1536-embedding']\n",
        "\n",
        "        # Create payload\n",
        "        payload = {\n",
        "            'text': example['text'],\n",
        "            'title': example['title'],\n",
        "            '_id': example['_id'],\n",
        "            'length': len(example['text']),\n",
        "            'has_numbers': any(char.isdigit() for char in example['text'])\n",
        "        }\n",
        "\n",
        "        points.append(models.PointStruct(\n",
        "            id=i,\n",
        "            vector=embedding,\n",
        "            payload=payload\n",
        "        ))\n",
        "\n",
        "    if points:\n",
        "        client.upload_points(collection_name=collection_name, points=points)\n",
        "        return len(points)\n",
        "    return 0\n",
        "\n",
        "# Upload all batches\n",
        "total_uploaded = 0\n",
        "for i in tqdm(range(0, total_points, batch_size), desc=\"Uploading points\"):\n",
        "    uploaded = upload_batch_without_indexes(i, i + batch_size)\n",
        "    total_uploaded += uploaded\n",
        "\n",
        "print(f\"\\nUpload completed! Total points uploaded: {total_uploaded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE7cMz2No7J6"
      },
      "source": [
        "### 7. Updating to HNSW Configuration\n",
        "\n",
        "Now we'll update the collection to use `m=16`, which builds HNSW connections between vectors. This should dramatically improve search speed by creating a navigable graph structure for vector search. Search becomes near‑logarithmic instead of linear scanning.\n",
        "\n",
        "The `m=16` parameter means each node connects to 16 nearest neighbors, creating a balance between search speed and index size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctwTIVX7o7J6",
        "outputId": "46227d8c-54b3-4cd8-9b2d-0eba10953984"
      },
      "outputs": [],
      "source": [
        "client.update_collection(\n",
        "    collection_name=collection_name,\n",
        "    hnsw_config=models.HnswConfigDiff(\n",
        "        m=16,  # Updated from 0 to 16\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"HNSW indexing enabled with m=16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvQvlE27tRvn"
      },
      "source": [
        "Now it will take some time to build the HNSW index. You can check whether it's fully built and optimized by, for example, looking at the **collection status** — once it changes from `YELLOW` to `GREEN`, the process is complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "KSwtVQKGtGx4",
        "outputId": "79f1d94f-1398-43d7-979a-871e87012d06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<CollectionStatus.GREEN: 'green'>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.get_collection(collection_name=collection_name).status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGIqod7Bt-rj"
      },
      "source": [
        "However, you can start querying the collection right away!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvf2b33po7J7"
      },
      "source": [
        "### 8. Creating a Test Query\n",
        "\n",
        "We're using OpenAI to generate an embedding for our query. We must use the same model (`text-embedding-3-large`) and dimensions (1536, first 1536 dimensions out of 3072) that were used to create our dataset embeddings.\n",
        "\n",
        "> We've provided the embedding of the test query for the experiments, so you **don't have to use the OpenAI API**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WJGsBS56o7J7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1536\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from materials.embedding_model import embed_model\n",
        "\n",
        "new_query = \"artificial intelligence\"\n",
        "\n",
        "def get_query_embedding(text):\n",
        "    try:\n",
        "        response = embed_model.embed_query(text)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting OpenAI embedding: {e}\")\n",
        "        print(\"Using random vector as fallback...\")\n",
        "        return np.random.normal(0, 1, 1536).tolist()\n",
        "\n",
        "# Get the embedding\n",
        "query_embedding = get_query_embedding(new_query)\n",
        "print(len(query_embedding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM_VrDdWvi_H",
        "outputId": "b64f5f2c-5a91-4328-e19d-33a6793eb1ef"
      },
      "outputs": [],
      "source": [
        "# Test query already embedded, if you prefer to avoid using OpenAI API\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://storage.googleapis.com/qdrant-examples/query_embedding_day_2.json\"\n",
        "resp = requests.get(url)\n",
        "\n",
        "query_embedding = resp.json()[\"query_vector\"]\n",
        "\n",
        "print(f\"Embedding dimensions: {len(query_embedding)}\")\n",
        "print(f\"First 5 values: {query_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SLgha-bo7J7"
      },
      "source": [
        "### 9. Performing Similarity Search\n",
        "\n",
        "After all vectors are indexed, we'll test the baseline performance with `m=16`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pdpitgOo7J7",
        "outputId": "3570ec17-f29d-4edc-edfa-a1d9c3749dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running baseline performance test...\n",
            "Warming up caches...\n",
            "Average search time: 345.55ms\n",
            "Search times: ['473.12ms', '283.73ms', '279.79ms']\n",
            "Found 10 results\n",
            "Top result: 'A.I. Artificial Intelligence' (score: 0.5392)\n"
          ]
        }
      ],
      "source": [
        "print(\"Running baseline performance test...\")\n",
        "\n",
        "# Warm up the RAM index/vectors cache with a test query\n",
        "print(\"Warming up caches...\")\n",
        "client.query_points(collection_name=collection_name, query=query_embedding, limit=1)\n",
        "\n",
        "# Measure vector search performance\n",
        "search_times = []\n",
        "for _ in range(3):  # Multiple runs for a stable average\n",
        "    start_time = time.time()\n",
        "    response = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_embedding,\n",
        "        limit=10\n",
        "    )\n",
        "    search_time = (time.time() - start_time) * 1000\n",
        "    search_times.append(search_time)\n",
        "\n",
        "baseline_time = sum(search_times) / len(search_times)\n",
        "\n",
        "print(f\"Average search time: {baseline_time:.2f}ms\")\n",
        "print(f\"Search times: {[f'{t:.2f}ms' for t in search_times]}\")\n",
        "print(f\"Found {len(response.points)} results\")\n",
        "print(f\"Top result: '{response.points[0].payload['title']}' (score: {response.points[0].score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGKGoJPOo7J7"
      },
      "source": [
        "The first time you run a query, Qdrant may need to load parts of the index from disk into memory, which can make it slower. After that, those parts stay cached in memory, so repeated queries are much faster. But if your machine is low on memory or you wait too long, the system might remove that cached data, and the process would repeat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmx-Pu-io7J7"
      },
      "source": [
        "### 10. Testing Filtering Without Indexes\n",
        "\n",
        "Now we'll test filtering performance without any payload indexes. This forces Qdrant to scan through all 100K vectors and check each one against the filter condition.\n",
        "\n",
        "We're comparing the search time with and without a filter to see the overhead of full scan filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyBSEumAo7J7",
        "outputId": "ba56b824-0e28-406c-f7fa-00f89c9068a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing filtering without payload indexes\n",
            "Filtered search (WITHOUT index): 942.11ms\n",
            "Individual times: ['1224.59ms', '780.72ms', '821.02ms']\n",
            "Overhead vs baseline: 596.56ms\n",
            "Found 10 matching results\n",
            "Top result: 'Cyc (/ˈsaɪk/) is an artificial intelligence project that attempts to assemble a comprehensive ontology and knowledge base of everyday common sense knowledge, with the goal of enabling AI applications to perform human-like reasoning.The project was started in 1984 by Douglas Lenat at MCC and is developed by the Cycorp company.Parts of the project are released as OpenCyc, which provides an API, RDF endpoint, and data dump under an open source license.'\n",
            "Score: 0.3030\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing filtering without payload indexes\")\n",
        "\n",
        "# Create a text-based filter\n",
        "text_filter = models.Filter(\n",
        "    must=[\n",
        "        models.FieldCondition(\n",
        "            key=\"text\",\n",
        "            match=models.MatchText(text=\"data\")\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Run multiple times for more reliable measurement\n",
        "unindexed_times = []\n",
        "for i in range(3):\n",
        "    start_time = time.time()\n",
        "    response = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_embedding,\n",
        "        limit=10,\n",
        "        search_params=models.SearchParams(hnsw_ef=100),\n",
        "        query_filter=text_filter\n",
        "    )\n",
        "    unindexed_times.append((time.time() - start_time) * 1000)\n",
        "\n",
        "unindexed_filter_time = sum(unindexed_times) / len(unindexed_times)\n",
        "\n",
        "print(f\"Filtered search (WITHOUT index): {unindexed_filter_time:.2f}ms\")\n",
        "print(f\"Individual times: {[f'{t:.2f}ms' for t in unindexed_times]}\")\n",
        "print(f\"Overhead vs baseline: {unindexed_filter_time - baseline_time:.2f}ms\")\n",
        "print(f\"Found {len(response.points)} matching results\")\n",
        "if response.points:\n",
        "    print(f\"Top result: '{response.points[0].payload['text']}'\\nScore: {response.points[0].score:.4f}\")\n",
        "else:\n",
        "    print(\"No results found - try a different filter term\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FVJQtL5o7J7"
      },
      "source": [
        "### 11. Creating Payload Indexes\n",
        "\n",
        "Now we'll create a [full text index](https://qdrant.tech/documentation/concepts/indexing/#full-text-index) on our text payload field. This should dramatically improve filtering performance by allowing Qdrant to quickly locate matching records instead of scanning all vectors.\n",
        "\n",
        "> **<font color='red'>Warning:</font>** The **Filterable HNSW** index (Qdrant’s native vector index structure designed for vector search with filtering) is built **only if** the payload indices are created **before** the HNSW index is built.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiHSdgiXo7J7",
        "outputId": "e356ac36-42ce-4a26-ba59-e4ef2f1fd936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Payload index created for 'text' field\n"
          ]
        }
      ],
      "source": [
        "client.create_payload_index(\n",
        "    collection_name=collection_name,\n",
        "    field_name=\"text\",\n",
        "    wait=True,\n",
        "    field_schema=models.TextIndexParams(\n",
        "        type=\"text\",\n",
        "        tokenizer=\"word\",\n",
        "        phrase_matching=False\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(\"Payload index created for 'text' field\")\n",
        "\n",
        "# If you want filter‑aware HNSW and you built the graph before creating payload indexes,\n",
        "# rebuild the graph to attach filter data structures.\n",
        "\n",
        "# Note: Reindexing takes up a lot of resources, and it is advised to set payload\n",
        "# indexes only once, before creating HNSW graph.\n",
        "# client.update_collection(collection_name=collection_name, hnsw_config=models.HnswConfigDiff(m=0))\n",
        "# client.update_collection(collection_name=collection_name, hnsw_config=models.HnswConfigDiff(m=16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT3g8FFoo7J7"
      },
      "source": [
        "### 12. Testing Filtering With Indexes\n",
        "\n",
        "Now we'll test the same filter query, but this time with the payload index in place. The performance difference should be dramatic - indexed filtering should be much faster than the full scan we just tested.\n",
        "\n",
        "This comparison shows the real-world impact of adding indexes to your vector search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MqV_csxo7J7",
        "outputId": "6c2af3f0-71e9-459c-8f10-ad45f400e6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing filtering WITH payload indexes...\n",
            "Warming up caches...\n",
            "Filtered search (WITH index): 324.50ms\n",
            "Individual times: ['412.68ms', '283.26ms', '277.56ms']\n",
            "Overhead vs baseline: -21.05ms\n",
            "Found 10 matching results\n",
            "Top result: 'Cyc (/ˈsaɪk/) is an artificial intelligence project that attempts to assemble a comprehensive ontology and knowledge base of everyday common sense knowledge, with the goal of enabling AI applications to perform human-like reasoning.The project was started in 1984 by Douglas Lenat at MCC and is developed by the Cycorp company.Parts of the project are released as OpenCyc, which provides an API, RDF endpoint, and data dump under an open source license.'\n",
            "Score: 0.3030\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing filtering WITH payload indexes...\")\n",
        "\n",
        "print(\"Warming up caches...\")\n",
        "client.query_points(collection_name=collection_name, query=query_embedding, limit=1)\n",
        "\n",
        "# Run multiple times for more reliable measurement\n",
        "indexed_times = []\n",
        "for i in range(3):\n",
        "    start_time = time.time()\n",
        "    response = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_embedding,\n",
        "        limit=10,\n",
        "        search_params=models.SearchParams(hnsw_ef=100),\n",
        "        query_filter=text_filter\n",
        "    )\n",
        "    indexed_times.append((time.time() - start_time) * 1000)\n",
        "\n",
        "indexed_filter_time = sum(indexed_times) / len(indexed_times)\n",
        "\n",
        "print(f\"Filtered search (WITH index): {indexed_filter_time:.2f}ms\")\n",
        "print(f\"Individual times: {[f'{t:.2f}ms' for t in indexed_times]}\")\n",
        "print(f\"Overhead vs baseline: {indexed_filter_time - baseline_time:.2f}ms\")\n",
        "print(f\"Found {len(response.points)} matching results\")\n",
        "if response.points:\n",
        "    print(f\"Top result: '{response.points[0].payload['text']}'\\nScore: {response.points[0].score:.4f}\")\n",
        "else:\n",
        "    print(\"No results found - try a different filter term\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzKKKeMVo7J7"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "In this tutorial, you've learned how to:\n",
        "\n",
        "- **Optimize initial upload speed** by starting with `m=0` and building HNSW later\n",
        "- **Measure filtering overhead** with and without payload indexes\n",
        "- **Tune our HNSW index and parameters**\n",
        "\n",
        "These techniques help you understand the performance trade-offs in vector search engines and optimize your applications for production use."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "materials",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
